{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer, WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re"
      ],
      "metadata": {
        "id": "K8PLPrGdkDju"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3cvd5DMYjRdn",
        "outputId": "80681303-9a00-46af-a3c6-fd53edde0364"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"The quick brown foxes are jumping over the lazy dogs. Writing and believing are two important skills.\""
      ],
      "metadata": {
        "id": "XeBLDbYLkOtG"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = word_tokenize(text)"
      ],
      "metadata": {
        "id": "JIV9u2EdkkJ2"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "porter = PorterStemmer()\n",
        "lancaster = LancasterStemmer()\n",
        "snowball = SnowballStemmer('english')"
      ],
      "metadata": {
        "id": "ikW79biMkob4"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def regex_stem(word):\n",
        "    pattern = re.compile(r'(ing|ly|ed|ious|ies|ive|es|s|ment)?$')\n",
        "    stem = re.sub(pattern, '', word)\n",
        "    return stem"
      ],
      "metadata": {
        "id": "AP-nABUTk4YO"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "def wordnet_lemmatize(word):\n",
        "    lemma = wordnet_lemmatizer.lemmatize(word, pos='v')\n",
        "    return lemma"
      ],
      "metadata": {
        "id": "F_Qq5rNflA84"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "porter_stemmed = [porter.stem(token) for token in tokens]\n",
        "lancaster_stemmed = [lancaster.stem(token) for token in tokens]\n",
        "regex_stemmed = [regex_stem(token) for token in tokens]\n",
        "snowball_stemmed = [snowball.stem(token) for token in tokens]\n",
        "wordnet_lemmatized = [wordnet_lemmatize(token) for token in tokens]"
      ],
      "metadata": {
        "id": "RmDdaJO9k90C"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Original Tokens:\", tokens)\n",
        "print(\"\\nPorter Stemmer:\", porter_stemmed)\n",
        "print(\"\\nLancaster Stemmer:\", lancaster_stemmed)\n",
        "print(\"\\nRegex Stemmer:\", regex_stemmed)\n",
        "print(\"\\nSnowball Stemmer:\", snowball_stemmed)\n",
        "print(\"\\nWordNet Lemmatizer:\", wordnet_lemmatized)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Dh_p6z3lS3r",
        "outputId": "ed5dd350-30f8-4986-941e-f67487049e05"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Tokens: ['The', 'quick', 'brown', 'foxes', 'are', 'jumping', 'over', 'the', 'lazy', 'dogs', '.', 'Writing', 'and', 'believing', 'are', 'two', 'important', 'skills', '.']\n",
            "\n",
            "Porter Stemmer: ['the', 'quick', 'brown', 'fox', 'are', 'jump', 'over', 'the', 'lazi', 'dog', '.', 'write', 'and', 'believ', 'are', 'two', 'import', 'skill', '.']\n",
            "\n",
            "Lancaster Stemmer: ['the', 'quick', 'brown', 'fox', 'ar', 'jump', 'ov', 'the', 'lazy', 'dog', '.', 'writ', 'and', 'believ', 'ar', 'two', 'import', 'skil', '.']\n",
            "\n",
            "Regex Stemmer: ['The', 'quick', 'brown', 'fox', 'are', 'jump', 'over', 'the', 'lazy', 'dog', '.', 'Writ', 'and', 'believ', 'are', 'two', 'important', 'skill', '.']\n",
            "\n",
            "Snowball Stemmer: ['the', 'quick', 'brown', 'fox', 'are', 'jump', 'over', 'the', 'lazi', 'dog', '.', 'write', 'and', 'believ', 'are', 'two', 'import', 'skill', '.']\n",
            "\n",
            "WordNet Lemmatizer: ['The', 'quick', 'brown', 'fox', 'be', 'jump', 'over', 'the', 'lazy', 'dog', '.', 'Writing', 'and', 'believe', 'be', 'two', 'important', 'skills', '.']\n"
          ]
        }
      ]
    }
  ]
}